{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dtm to visualize Endrew v. Douglas County briefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/raj2/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shogun\n",
    "from shogun import StringCharFeatures, RAWBYTE\n",
    "from shogun import BinaryLabels\n",
    "from shogun import SubsequenceStringKernel\n",
    "from shogun import LibSVM\n",
    "import spacy\n",
    "!python -m spacy download en\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import numpy as np  \n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "from heapq import nlargest as _nlargest\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## string cleaning\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "## add punctuation and some application-specific words\n",
    "## to stopword list\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', \n",
    "                   '(', ')', '[', ']', '{', '}', 'iep','idea']) # \n",
    "porter = PorterStemmer()\n",
    "\n",
    "## sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "## link processing\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded imports successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded imports successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general-purpose string/text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function (from stackoverflow)\n",
    "## that\n",
    "## takes in:\n",
    "## @path: pathname\n",
    "## returns:\n",
    "## string\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "def lower_removepunct(string_toclean):\n",
    "    string_lower = string_toclean.lower()\n",
    "    string_lower_char = \"\".join(x for x in string_lower if x not in string.punctuation) \n",
    "    return(string_lower_char)\n",
    "\n",
    "occurrences = lambda s, lst: (i for i,e in enumerate(lst) if s in e)\n",
    "\n",
    "def get_close_matches_indexes(word, possibilities, n=3, cutoff=0.6):\n",
    "    \"\"\"Use SequenceMatcher to return a list of the indexes of the best \n",
    "    \"good enough\" matches. word is a sequence for which close matches \n",
    "    are desired (typically a string).\n",
    "    possibilities is a list of sequences against which to match word\n",
    "    (typically a list of strings).\n",
    "    Optional arg n (default 3) is the maximum number of close matches to\n",
    "    return.  n must be > 0.\n",
    "    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n",
    "    that don't score at least that similar to word are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    if not n >  0:\n",
    "        raise ValueError(\"n must be > 0: %r\" % (n,))\n",
    "    if not 0.0 <= cutoff <= 1.0:\n",
    "        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n",
    "    result = []\n",
    "    s = SequenceMatcher()\n",
    "    s.set_seq2(word)\n",
    "    for idx, x in enumerate(possibilities):\n",
    "        s.set_seq1(x)\n",
    "        if s.real_quick_ratio() >= cutoff and s.quick_ratio() >= cutoff and s.ratio() >= cutoff:\n",
    "            result.append((s.ratio(), idx))\n",
    "\n",
    "    # Move the best scorers to head of list\n",
    "    result = _nlargest(n, result)\n",
    "\n",
    "    # Strip scores for the best n matches\n",
    "    return [x for score, x in result]\n",
    "\n",
    "## function for dtm representation\n",
    "def create_nonmasked_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "    dtm_dense_named_withid = pd.concat([metadata, dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)\n",
    "\n",
    "def create_sentiment_dataframe(list_ofsentiment_dicts, metadata):\n",
    "    sentiment_df = pd.DataFrame.from_records(list_ofsentiment_dicts).fillna(0)\n",
    "    sentiment_df.columns = ['sentimentsummary_'+ str(i) for i in sentiment_df.columns]\n",
    "    sentiment_withid = pd.concat([metadata, sentiment_df], axis = 1)\n",
    "    return(sentiment_withid)\n",
    "\n",
    "def create_distance_matrix(full_essays, ids):\n",
    "    string_features = StringCharFeatures(full_essays, RAWBYTE)\n",
    "    sk = SubsequenceStringKernel(string_features, string_features, 3, 0.5)\n",
    "    sk_matrix = sk.get_kernel_matrix()\n",
    "    sk_df = pd.DataFrame(sk_matrix)\n",
    "    sk_df.columns = ['id_'+ str(i) for i in ids]\n",
    "    return(sk_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for case processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_links_todocs(url, prefix = None):\n",
    "    \n",
    "    ## call link and open\n",
    "    request = urllib.request.urlopen(url)\n",
    "    opened_page = request.read()\n",
    "    \n",
    "    ## parse page and find all links\n",
    "    parsed_page = BeautifulSoup(opened_page, \"lxml\")\n",
    "    links_page = parsed_page.findAll('a')\n",
    "    \n",
    "    ## iterate over links and return all\n",
    "    all_links = [a['href'] for a in links_page]\n",
    "    \n",
    "    ## if there's a prefix to subset by, add that\n",
    "    if prefix is not None:\n",
    "        prefix_links = [link for link in all_links if link.startswith(prefix)]\n",
    "        \n",
    "        return(prefix_links)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return(all_links)\n",
    "    \n",
    "## for each case, create dataframe\n",
    "def get_metadata_andtext(parsed_page):\n",
    "    \n",
    "    case_metadata = parsed_page.find_all('meta', attrs = {'name': True})\n",
    "    what_to_extract = ['docket', 'decided', 'caption', 'judge', 'summary']\n",
    "    case_metadata_content = [tag['content'] for tag in case_metadata if tag['name'] in what_to_extract]\n",
    "    case_dict = dict(zip(what_to_extract, case_metadata_content))\n",
    "    case_text = parsed_page.find('p').getText()\n",
    "    case_dict['full_text'] = case_text\n",
    "    case_dict_df = pd.DataFrame(case_dict, index = [0])\n",
    "    return(case_dict_df)\n",
    "    \n",
    "def split_caption(data, nameof_captioncol):\n",
    "    \n",
    "    caption_split = data[nameof_captioncol].str.split('v.', 1).tolist()\n",
    "    plaintiff = [item[0] if isinstance(item, list) else 'Bad split' for item in caption_split]\n",
    "    defendant = [item[1] if isinstance(item, list) and len(item) > 1 else 'Bad split' for item in caption_split]\n",
    "    return(plaintiff, defendant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one: create document-term matrix of briefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_briefs_parents = '/Users/raj2/Dropbox/endrewamicus_textanalysis/parents/'\n",
    "os.chdir(path_to_briefs_parents)\n",
    "current_path = os.getcwd()\n",
    "briefs = glob.glob('*pdf*')\n",
    "briefs_paths_parents = [path_to_briefs_parents + \"/\" + brief for brief in briefs]\n",
    "path_to_briefs_district = '/Users/raj2/Dropbox/endrewamicus_textanalysis/district/'\n",
    "os.chdir(path_to_briefs_district)\n",
    "current_path = os.getcwd()\n",
    "briefs = glob.glob('*pdf*')\n",
    "briefs_paths_district = [path_to_briefs_district + \"/\" + brief for brief in briefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827-amicus-respondent-AASA-the-School-Superintendents-Association-et-al.pdf',\n",
       " '/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827-amicus-respondent-CGCS.pdf',\n",
       " '/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827_amicus_resp_colorado_state_board_of_education.pdf',\n",
       " '/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827_amicus_resp_NSBA.pdf']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefs_paths_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-Advocates-for-Children-of-New-York-et-al.pdf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_brief = briefs_paths[1]\n",
    "one_brief\n",
    "\n",
    "amicus_alltext = convert_pdf_to_txt(one_brief)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_amicus_df(path):\n",
    "    \n",
    "    amicus_alltext = convert_pdf_to_txt(path)\n",
    "    \n",
    "    ## split amicus to get summary\n",
    "    amicus_split = amicus_alltext.splitlines()\n",
    "\n",
    "    ## pull the index of the start and ends of the brief\n",
    "    arg_start = 'SUMMARY OF ARGUMENT'\n",
    "    arg_start_all = [amicus_split.index(i) for i in amicus_split if arg_start in i]\n",
    "    if len(arg_start_all) == 0:\n",
    "        arg_start_index = 0\n",
    "    elif len(arg_start_all) == 1:\n",
    "        arg_start_index = arg_start_all[0]\n",
    "    else:\n",
    "        arg_start_index = arg_start_all[1]\n",
    "\n",
    "    arg_end = 'Respectfully submitted'\n",
    "    arg_end_index = min([amicus_split.index(i) for i in amicus_split if arg_end in i])\n",
    "    \n",
    "    ## extract the brief argument\n",
    "    amicus_arg = \" \".join(amicus_split[arg_start_index+1:arg_end_index])\n",
    "\n",
    "    ## return stemmed/lemmatized \n",
    "    amicus_stemmed = \" \".join([porter.stem(i.lower()) for i in wordpunct_tokenize(amicus_arg) if \n",
    "                        i.lower() not in stop_words and i.lower().isalpha()])\n",
    "    \n",
    "    return(amicus_stemmed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-118-members-of-congress.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-Advocates-for-Children-of-New-York-et-al.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-Council_of_Parent_Attorneys_and_Advocates.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-delaware.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-FormerU.S.DeptofEduc.Officials.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-National-Education-Association.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-NDRN.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-amicus-petitioner-The-Coalition-of-Texans.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//15-827-pet-amicus-NCSECS.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//Disability-Rights-Organizations-Endrew-amicus.pdf\n",
      "returned amicus\n"
     ]
    }
   ],
   "source": [
    "all_parents = []\n",
    "all_districts = []\n",
    "for i in range(0, len(briefs_paths_parents)):\n",
    "    \n",
    "    one_path = briefs_paths_parents[i]\n",
    "    print(one_path)\n",
    "    one_amicus = return_amicus_df(one_path)\n",
    "    print('returned amicus')\n",
    "    all_parents.append(one_amicus)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827-amicus-respondent-AASA-the-School-Superintendents-Association-et-al.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827-amicus-respondent-CGCS.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827_amicus_resp_colorado_state_board_of_education.pdf\n",
      "returned amicus\n",
      "/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827_amicus_resp_NSBA.pdf\n",
      "returned amicus\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(briefs_paths_district)):\n",
    "    \n",
    "    one_path = briefs_paths_district[i]\n",
    "    print(one_path)\n",
    "    one_amicus = return_amicus_df(one_path)\n",
    "    print('returned amicus')\n",
    "    all_districts.append(one_amicus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## name of amicus petitioners \n",
    "parent_briefs_init = [re.sub(\"/Users/raj2/Dropbox/endrewamicus_textanalysis/parents//(15-827-amicus-petitioner-)?\", \"\", x) \n",
    "                 for x in briefs_paths_parents]\n",
    "parent_briefs_final = [re.sub(\".pdf\", \"\", x) \n",
    "                 for x in parent_briefs_init]\n",
    "\n",
    "district_briefs_init = [re.sub(\"/Users/raj2/Dropbox/endrewamicus_textanalysis/district//15-827.amicus.resp(ondent)?[\\\\-|\\\\_]\", \"\", x) \n",
    "                 for x in briefs_paths_district]\n",
    "district_briefs_final = [re.sub(\".pdf\", \"\", x) \n",
    "                 for x in district_briefs_init]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org</th>\n",
       "      <th>who_filed_for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118-members-of-congress</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advocates-for-Children-of-New-York-et-al</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Council_of_Parent_Attorneys_and_Advocates</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delaware</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FormerU.S.DeptofEduc.Officials</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>National-Education-Association</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDRN</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The-Coalition-of-Texans</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15-827-pet-amicus-NCSECS</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Disability-Rights-Organizations-Endrew-amicus</td>\n",
       "      <td>parent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AASA-the-School-Superintendents-Association-et-al</td>\n",
       "      <td>district</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CGCS</td>\n",
       "      <td>district</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>colorado_state_board_of_education</td>\n",
       "      <td>district</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NSBA</td>\n",
       "      <td>district</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  org who_filed_for\n",
       "0                             118-members-of-congress        parent\n",
       "1            Advocates-for-Children-of-New-York-et-al        parent\n",
       "2           Council_of_Parent_Attorneys_and_Advocates        parent\n",
       "3                                            delaware        parent\n",
       "4                      FormerU.S.DeptofEduc.Officials        parent\n",
       "5                      National-Education-Association        parent\n",
       "6                                                NDRN        parent\n",
       "7                             The-Coalition-of-Texans        parent\n",
       "8                            15-827-pet-amicus-NCSECS        parent\n",
       "9       Disability-Rights-Organizations-Endrew-amicus        parent\n",
       "10  AASA-the-School-Superintendents-Association-et-al      district\n",
       "11                                               CGCS      district\n",
       "12                  colorado_state_board_of_education      district\n",
       "13                                               NSBA      district"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filed_for = ['parent'] * len(parent_briefs_final) + ['district'] * len(district_briefs_final)\n",
    "\n",
    "amicus_id = pd.DataFrame({'org': parent_briefs_final + district_briefs_final,\n",
    "                             'who_filed_for': filed_for})\n",
    "amicus_id\n",
    "\n",
    "amicus_dtm = create_nonmasked_dtm(list_of_strings=all_parents + all_districts, metadata = amicus_id)\n",
    "\n",
    "amicus_dtm.to_csv('/Users/raj2/Dropbox/endrewamicus_textanalysis/output/endrew_amicus_dtm.csv',\n",
    "                  index = False)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
